{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, such as subprocess, to run external scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import subprocess  # To run external scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Preprocessing Pipeline Script\n",
    "Download the dataset from Kaggle, extract the dataset and execute the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully from '/home/smebellis/ece5831_demo/config.yaml'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 17:12:29 - root - setup_logging - INFO - Logging initialized. Logs are being saved to logs/app.log\n",
      "2024-12-03 17:12:33 - __main__ - main - INFO - Starting Preprocessing Pipeline.\n",
      "2024-12-03 17:12:33 - __main__ - main - INFO - Loading district boundary data...\n",
      "2024-12-03 17:12:33 - src.districts - load_districts - INFO - Loading district boundary data...\n",
      "2024-12-03 17:12:33 - src.helper - download_dataset - INFO - data/taxi-trajectory.zip not found. Downloading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/crailtap/taxi-trajectory\n",
      "License(s): unknown\n",
      "Downloading taxi-trajectory.zip to data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515M/515M [00:30<00:00, 18.0MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 17:13:14 - src.helper - download_dataset - INFO - Dataset downloaded and extracted to data.\n",
      "2024-12-03 17:13:14 - __main__ - main - INFO - Directory created at processed_data\n",
      "2024-12-03 17:13:14 - src.pipeline - preprocessing_pipeline - INFO - Loading data from data/train.csv\n",
      "Reading CSV: 100%|█████████▉| 1710670/1710671 [00:12<00:00, 133996.59lines/s]\n",
      "2024-12-03 17:13:28 - src.pipeline - preprocessing_pipeline - INFO - Loaded data from data/train.csv.\n",
      "2024-12-03 17:13:28 - src.pipeline - preprocessing_pipeline - INFO - Removing rows with missing GPS data.\n",
      "2024-12-03 17:13:29 - src.pipeline - preprocessing_pipeline - INFO - Dropping unnecessary columns from DataFrame.\n",
      "2024-12-03 17:13:29 - src.pipeline - preprocessing_pipeline - INFO - Converting UNIX timestamps to datetime objects.\n",
      "2024-12-03 17:13:29 - src.pipeline - preprocessing_pipeline - INFO - Parsing and correcting POLYLINE Coordinates\n",
      "Parsing and correcting POLYLINE coordinates: 100%|██████████| 1710660/1710660 [00:46<00:00, 37002.90it/s]\n",
      "2024-12-03 17:14:16 - src.pipeline - preprocessing_pipeline - INFO - Extracting Starting and Ending locations from Polyline column.\n",
      "Extracting Coordinates: 100%|██████████| 1710660/1710660 [00:01<00:00, 898623.97it/s] \n",
      "Extracting Coordinates: 100%|██████████| 1710660/1710660 [00:01<00:00, 929871.27it/s] \n",
      "Extracting Coordinates: 100%|██████████| 1710660/1710660 [00:01<00:00, 916544.58it/s] \n",
      "Extracting Coordinates: 100%|██████████| 1710660/1710660 [00:01<00:00, 904930.13it/s] \n",
      "Extracting Coordinates: 100%|██████████| 1710660/1710660 [00:12<00:00, 133693.49it/s]\n",
      "2024-12-03 17:14:37 - src.pipeline - preprocessing_pipeline - INFO - Removing rows with NaN data.\n",
      "2024-12-03 17:14:37 - src.Preprocessing - drop_nan - INFO - Initiating drop_na operation.\n",
      "2024-12-03 17:14:37 - src.Preprocessing - drop_nan - INFO - Dropped 5901 rows containing NaN values.\n",
      "2024-12-03 17:14:37 - src.pipeline - preprocessing_pipeline - INFO - Separating time into individual components\n",
      "2024-12-03 17:14:38 - src.pipeline - preprocessing_pipeline - INFO - Assigning districts to taxi data.\n",
      "2024-12-03 17:14:38 - src.Preprocessing - assign_district_vectorized - INFO - Starting vectorized district assignment.\n",
      "Assigning districts: 100%|██████████| 19/19 [00:01<00:00, 11.75it/s]\n",
      "2024-12-03 17:14:40 - src.Preprocessing - assign_district_vectorized - INFO - Vectorized district assignment completed.\n",
      "2024-12-03 17:14:40 - src.pipeline - preprocessing_pipeline - INFO - Calculate the travel time\n",
      "Calculating Travel Time: 100%|██████████| 1704759/1704759 [00:01<00:00, 1393652.73it/s]\n",
      "2024-12-03 17:14:41 - src.pipeline - preprocessing_pipeline - INFO - Calculate Trip Distance\n",
      "2024-12-03 17:14:41 - src.Preprocessing - calculate_trip_distance - INFO - Starting trip distance calculation.\n",
      "2024-12-03 17:14:41 - src.Preprocessing - calculate_trip_distance - INFO - Calculating distance for each polyline using vectorized approach.\n",
      "Calculating Trip Distance: 100%|██████████| 1704759/1704759 [00:56<00:00, 30269.41it/s]\n",
      "2024-12-03 17:15:37 - src.Preprocessing - calculate_trip_distance - INFO - Trip distance calculation completed.\n",
      "2024-12-03 17:15:37 - src.pipeline - preprocessing_pipeline - INFO - Calculate the Average speed\n",
      "2024-12-03 17:15:37 - src.Preprocessing - calculate_avg_speed - INFO - Starting average speed calculation.\n",
      "2024-12-03 17:15:37 - src.Preprocessing - calculate_avg_speed - INFO - Calculating average speed for valid travel times.\n",
      "2024-12-03 17:15:38 - src.Preprocessing - calculate_avg_speed - INFO - Average speed calculation completed.\n",
      "2024-12-03 17:18:51 - src.pipeline - preprocessing_pipeline - INFO - File saved to processed_data/taxi_data_processed.csv.\n",
      "2024-12-03 17:18:51 - src.pipeline - preprocessing_pipeline - INFO - Preprocessing pipeline completed successfully.\n",
      "2024-12-03 17:18:57 - __main__ - main - INFO - Preprocessing Pipeline Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'scripts/run_preprocessing_pipeline.py'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Preprocessing Pipeline Script\n",
    "subprocess.run([\"python\", \"scripts/run_preprocessing_pipeline.py\"], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Cluster Districts Script\n",
    "Execute the clustering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully from '/home/smebellis/ece5831_demo/config.yaml'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 17:35:50 - root - setup_logging - INFO - Logging initialized. Logs are being saved to logs/app.log\n",
      "2024-12-03 17:35:52 - __main__ - main - INFO - Starting Clustering Pipeline.\n",
      "2024-12-03 17:35:52 - __main__ - main - INFO - Loading district boundary data...\n",
      "2024-12-03 17:35:52 - src.districts - load_districts - INFO - Loading district boundary data...\n",
      "Reading CSV: 100%|█████████▉| 1704759/1704760 [00:25<00:00, 67220.55lines/s]\n",
      "2024-12-03 17:36:24 - __main__ - main - INFO - CSV file read successfully.\n",
      "2024-12-03 17:36:24 - src.cluster_districts - cluster_trip_district - INFO - Defining district centroids.\n",
      "2024-12-03 17:36:24 - src.cluster_districts - cluster_trip_district - INFO - Applying KMeans clustering to group districts into 7 clusters.\n",
      "2024-12-03 17:36:24 - src.cluster_districts - cluster_trip_district - INFO - Assigning cluster names.\n",
      "2024-12-03 17:36:24 - src.cluster_districts - cluster_trip_district - INFO - Mapping each district in the dataframe to its corresponding cluster.\n",
      "2024-12-03 17:36:24 - src.cluster_districts - cluster_trip_district - INFO - District clustering completed.\n",
      "2024-12-03 17:36:24 - src.cluster_districts - cluster_trip_time - INFO - Reshaping the TIME column for KMeans clustering.\n",
      "2024-12-03 17:36:24 - src.cluster_districts - cluster_trip_time - INFO - Applying KMeans clustering with 3 clusters (morning, afternoon, night).\n",
      "2024-12-03 17:36:24 - src.cluster_districts - cluster_trip_time - INFO - Getting cluster centers and sorting them to define time intervals.\n",
      "2024-12-03 17:36:24 - src.cluster_districts - cluster_trip_time - INFO - Cluster centers (time intervals): [4.621782178217817, 12.00208659363589, 19.088647959183668]\n",
      "2024-12-03 17:36:24 - src.cluster_districts - cluster_trip_time - INFO - Time clustering completed.\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Starting clustering process for grouped data.\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=0\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=1\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=2\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=3\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=4\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=5\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=6\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=7\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=8\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=9\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=10\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=11\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=12\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=13\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=14\n",
      "2024-12-03 17:36:24 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=15\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=16\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=17\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=18\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=19\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=20\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=21\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=22\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Friday, TIME=23\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=0\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=1\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=2\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=3\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=4\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=5\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=6\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=7\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=8\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=9\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=10\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=11\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=12\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=13\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=14\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=15\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=16\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=17\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=18\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=19\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=20\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=21\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=22\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Monday, TIME=23\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=0\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=1\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=2\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=3\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=4\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=5\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=6\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=7\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=8\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=9\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=10\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=11\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=12\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=13\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=14\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=15\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=16\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=17\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=18\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=19\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=20\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=21\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=22\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Saturday, TIME=23\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=0\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=1\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=2\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=3\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=4\n",
      "2024-12-03 17:36:25 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=5\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=6\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=7\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=8\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=9\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=10\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=11\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=12\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=13\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=14\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=15\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=16\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=17\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=18\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=19\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=20\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=21\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=22\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Sunday, TIME=23\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=0\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=1\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=2\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=3\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=4\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=5\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=6\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=7\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=8\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=9\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=10\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=11\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=12\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=13\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=14\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=15\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=16\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=17\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=18\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=19\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=20\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=21\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=22\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Thursday, TIME=23\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=0\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=1\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=2\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=3\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=4\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=5\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=6\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=7\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=8\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=9\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=10\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=11\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=12\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=13\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=14\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=15\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=16\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=17\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=18\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=19\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=20\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=21\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=22\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Tuesday, TIME=23\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=0\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=1\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=2\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=3\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=4\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=5\n",
      "2024-12-03 17:36:26 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=6\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=7\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=8\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=9\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=10\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=11\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=12\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=13\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=14\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=15\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=16\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=17\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=18\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=19\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=20\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=21\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=22\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Processing group: WEEKDAY=Wednesday, TIME=23\n",
      "2024-12-03 17:36:27 - src.cluster_districts - HDBSCAN_Clustering_Aggregated_optimized - INFO - Clustering process completed.\n",
      "2024-12-03 17:36:27 - src.cluster_districts - determine_traffic_status - INFO - Determining traffic status based on DOM values and thresholds.\n",
      "2024-12-03 17:36:27 - src.cluster_districts - determine_traffic_status - INFO - Traffic status determination completed.\n",
      "2024-12-03 17:36:27 - src.cluster_districts - encode_geographical_context - INFO - Encoding geographical context by clustering start and end coordinates.\n",
      "2024-12-03 17:36:27 - src.cluster_districts - encode_geographical_context - INFO - Geographical context encoding completed.\n",
      "2024-12-03 17:36:27 - src.cluster_districts - aggregate_district_clusters - INFO - Aggregating district and regional clusters into a composite feature.\n",
      "2024-12-03 17:36:27 - src.cluster_districts - aggregate_district_clusters - INFO - District and regional cluster aggregation completed.\n",
      "2024-12-03 17:36:27 - src.cluster_districts - traffic_congestion_indicator - INFO - Calculating Traffic congestion\n",
      "2024-12-03 17:36:27 - src.cluster_districts - traffic_congestion_indicator - INFO - Traffic congestion indicator calculation completed.\n",
      "2024-12-03 17:36:27 - src.cluster_districts - add_temporal_context - INFO - Adding temporal context to the combined cluster feature.\n",
      "2024-12-03 17:36:27 - src.cluster_districts - add_temporal_context - INFO - Temporal context added to combined cluster feature.\n",
      "2024-12-03 17:36:27 - root - save_dataframe_overwrite - INFO - File saved to processed_data/clustered_data.csv. (Overwritten if it existed)\n",
      "2024-12-03 17:36:27 - __main__ - main - INFO - Clustered DataFrame Save to processed_data/clustered_data.csv\n",
      "2024-12-03 17:36:27 - __main__ - main - INFO - Completed Clustering Pipeline...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'scripts/run_cluster_districts.py'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Cluster Districts Script\n",
    "subprocess.run([\"python\", \"scripts/run_cluster_districts.py\"], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Feature Engineer Script\n",
    "Engineer the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully from '/home/smebellis/ece5831_demo/config.yaml'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 17:36:37 - root - setup_logging - INFO - Logging initialized. Logs are being saved to logs/app.log\n",
      "Reading CSV: 100%|█████████▉| 4917/4918 [00:00<00:00, 67484.70lines/s]\n",
      "2024-12-03 17:36:37 - __main__ - <module> - INFO - Loaded original dataset from CSV.\n",
      "2024-12-03 17:36:37 - __main__ - <module> - INFO - Dropped the following columns: ['TRIP_ID', 'ROUTE', 'CALL_TYPE', 'TAXI_ID', 'DAY_TYPE']\n",
      "2024-12-03 17:36:37 - __main__ - <module> - INFO - Using Swifter to convert POLYLINE to list\n",
      "Pandas Apply: 100%|██████████| 4917/4917 [00:01<00:00, 3874.50it/s]\n",
      "2024-12-03 17:36:38 - __main__ - <module> - INFO - Completed POLYLINE to list conversion\n",
      "2024-12-03 17:36:38 - __main__ - <module> - INFO - Spliting the dataset into training, validation, and test sets.\n",
      "2024-12-03 17:36:38 - __main__ - <module> - INFO - Completed splitting the dataset into training, validation, and test sets.\n",
      "2024-12-03 17:36:38 - __main__ - save_as_pkl - INFO - train.pkl saved successfully at pickle_files/train.pkl\n",
      "2024-12-03 17:36:38 - __main__ - save_as_pkl - INFO - val.pkl saved successfully at pickle_files/val.pkl\n",
      "2024-12-03 17:36:38 - __main__ - save_as_pkl - INFO - test.pkl saved successfully at pickle_files/test.pkl\n",
      "2024-12-03 17:36:38 - src.FeatureEngineering - __init__ - INFO - Initialized FeatureEngineeringPipeline with scaler, imputers, encoders, and PCA.\n",
      "2024-12-03 17:36:38 - __main__ - <module> - INFO - Fitting the feature engineering pipeline on the training set.\n",
      "2024-12-03 17:36:38 - src.FeatureEngineering - fit - INFO - Fitting imputers, encoders, and scaler on the training data.\n",
      "2024-12-03 17:36:38 - src.FeatureEngineering - fit - INFO - Completed fitting encoders, scaler, and PCA.\n",
      "2024-12-03 17:36:38 - __main__ - <module> - INFO - Created directory: preprocessed_tensors\n",
      "2024-12-03 17:36:38 - __main__ - <module> - INFO - Encoded labels in training set: [0 1 2]\n",
      "2024-12-03 17:36:38 - __main__ - <module> - INFO - Encoded labels in validation set: [0 1 2]\n",
      "2024-12-03 17:36:38 - __main__ - <module> - INFO - Encoded labels in test set: [0 1 2]\n",
      "2024-12-03 17:36:38 - __main__ - <module> - INFO - LabelEncoder saved at 'pickle_files/label_encoder.pkl'.\n",
      "2024-12-03 17:36:38 - __main__ - batch_process_pipeline - INFO - Batch processing train set with batch size of 1000.\n",
      "2024-12-03 17:36:38 - __main__ - batch_process_pipeline - INFO - Processing batch 0 to 1000 for train set.\n",
      "2024-12-03 17:36:38 - src.FeatureEngineering - transform - INFO - Starting transformation of the dataframe.\n",
      "2024-12-03 17:36:38 - src.FeatureEngineering - transform - INFO - Converting POLYLINE to image representations.\n",
      "2024-12-03 17:36:39 - src.FeatureEngineering - transform - INFO - Concatenating all engineered features.\n",
      "2024-12-03 17:36:39 - src.FeatureEngineering - transform - INFO - Converting features to torch tensors and normalize tensors.\n",
      "2024-12-03 17:36:39 - src.FeatureEngineering - transform - INFO - Transformation complete.\n",
      "2024-12-03 17:36:39 - __main__ - batch_process_pipeline - INFO - Saved batch 0 to 1000 for train set.\n",
      "2024-12-03 17:36:39 - __main__ - batch_process_pipeline - INFO - Processing batch 1000 to 2000 for train set.\n",
      "2024-12-03 17:36:39 - src.FeatureEngineering - transform - INFO - Starting transformation of the dataframe.\n",
      "2024-12-03 17:36:39 - src.FeatureEngineering - transform - INFO - Converting POLYLINE to image representations.\n",
      "2024-12-03 17:36:40 - src.FeatureEngineering - transform - INFO - Concatenating all engineered features.\n",
      "2024-12-03 17:36:40 - src.FeatureEngineering - transform - INFO - Converting features to torch tensors and normalize tensors.\n",
      "2024-12-03 17:36:40 - src.FeatureEngineering - transform - INFO - Transformation complete.\n",
      "2024-12-03 17:36:40 - __main__ - batch_process_pipeline - INFO - Saved batch 1000 to 2000 for train set.\n",
      "2024-12-03 17:36:40 - __main__ - batch_process_pipeline - INFO - Processing batch 2000 to 3000 for train set.\n",
      "2024-12-03 17:36:40 - src.FeatureEngineering - transform - INFO - Starting transformation of the dataframe.\n",
      "2024-12-03 17:36:40 - src.FeatureEngineering - transform - INFO - Converting POLYLINE to image representations.\n",
      "2024-12-03 17:36:41 - src.FeatureEngineering - transform - INFO - Concatenating all engineered features.\n",
      "2024-12-03 17:36:41 - src.FeatureEngineering - transform - INFO - Converting features to torch tensors and normalize tensors.\n",
      "2024-12-03 17:36:41 - src.FeatureEngineering - transform - INFO - Transformation complete.\n",
      "2024-12-03 17:36:41 - __main__ - batch_process_pipeline - INFO - Saved batch 2000 to 3000 for train set.\n",
      "2024-12-03 17:36:41 - __main__ - batch_process_pipeline - INFO - Processing batch 3000 to 3933 for train set.\n",
      "2024-12-03 17:36:41 - src.FeatureEngineering - transform - INFO - Starting transformation of the dataframe.\n",
      "2024-12-03 17:36:41 - src.FeatureEngineering - transform - INFO - Converting POLYLINE to image representations.\n",
      "2024-12-03 17:36:42 - src.FeatureEngineering - transform - INFO - Concatenating all engineered features.\n",
      "2024-12-03 17:36:42 - src.FeatureEngineering - transform - INFO - Converting features to torch tensors and normalize tensors.\n",
      "2024-12-03 17:36:42 - src.FeatureEngineering - transform - INFO - Transformation complete.\n",
      "2024-12-03 17:36:42 - __main__ - batch_process_pipeline - INFO - Saved batch 3000 to 3933 for train set.\n",
      "2024-12-03 17:36:42 - __main__ - batch_process_pipeline - INFO - Batch processing val set with batch size of 1000.\n",
      "2024-12-03 17:36:42 - __main__ - batch_process_pipeline - INFO - Processing batch 0 to 492 for val set.\n",
      "2024-12-03 17:36:42 - src.FeatureEngineering - transform - INFO - Starting transformation of the dataframe.\n",
      "2024-12-03 17:36:42 - src.FeatureEngineering - transform - INFO - Converting POLYLINE to image representations.\n",
      "2024-12-03 17:36:43 - src.FeatureEngineering - transform - INFO - Concatenating all engineered features.\n",
      "2024-12-03 17:36:43 - src.FeatureEngineering - transform - INFO - Converting features to torch tensors and normalize tensors.\n",
      "2024-12-03 17:36:43 - src.FeatureEngineering - transform - INFO - Transformation complete.\n",
      "2024-12-03 17:36:43 - __main__ - batch_process_pipeline - INFO - Saved batch 0 to 492 for val set.\n",
      "2024-12-03 17:36:43 - __main__ - batch_process_pipeline - INFO - Batch processing test set with batch size of 1000.\n",
      "2024-12-03 17:36:43 - __main__ - batch_process_pipeline - INFO - Processing batch 0 to 492 for test set.\n",
      "2024-12-03 17:36:43 - src.FeatureEngineering - transform - INFO - Starting transformation of the dataframe.\n",
      "2024-12-03 17:36:43 - src.FeatureEngineering - transform - INFO - Converting POLYLINE to image representations.\n",
      "2024-12-03 17:36:43 - src.FeatureEngineering - transform - INFO - Concatenating all engineered features.\n",
      "2024-12-03 17:36:43 - src.FeatureEngineering - transform - INFO - Converting features to torch tensors and normalize tensors.\n",
      "2024-12-03 17:36:43 - src.FeatureEngineering - transform - INFO - Transformation complete.\n",
      "2024-12-03 17:36:43 - __main__ - batch_process_pipeline - INFO - Saved batch 0 to 492 for test set.\n",
      "2024-12-03 17:36:43 - __main__ - <module> - INFO - Batch processing complete. All tensors saved successfully.\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Concatenating all train batch files into a single tensor.\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Successfully saved concatenated train tensors as single .pt files.\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleting batch files for train set.\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_additional_features_tensor_0_1000.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_additional_features_tensor_1000_2000.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_additional_features_tensor_2000_3000.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_additional_features_tensor_3000_3933.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_labels_tensor_0_1000.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_labels_tensor_1000_2000.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_labels_tensor_2000_3000.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_labels_tensor_3000_3933.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_route_images_tensor_0_1000.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_route_images_tensor_1000_2000.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_route_images_tensor_2000_3000.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/train_route_images_tensor_3000_3933.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Concatenating all val batch files into a single tensor.\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Successfully saved concatenated val tensors as single .pt files.\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleting batch files for val set.\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/val_additional_features_tensor_0_492.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/val_labels_tensor_0_492.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/val_route_images_tensor_0_492.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Concatenating all test batch files into a single tensor.\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Successfully saved concatenated test tensors as single .pt files.\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleting batch files for test set.\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/test_additional_features_tensor_0_492.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/test_labels_tensor_0_492.pt\n",
      "2024-12-03 17:36:43 - __main__ - concatenate_and_save - INFO - Deleted batch file: preprocessed_tensors/test_route_images_tensor_0_492.pt\n",
      "2024-12-03 17:36:43 - __main__ - <module> - INFO - All batches concatenated and saved as single .pt files successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'scripts/run_feature_engineering_pipeline.py'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Feature Engineer Script\n",
    "subprocess.run([\"python\", \"scripts/run_feature_engineering_pipeline.py\"], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training Script\n",
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully from '/home/smebellis/ece5831_demo/config.yaml'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 17:36:52 - root - setup_logging - INFO - Logging initialized. Logs are being saved to logs/app.log\n",
      "2024-12-03 17:36:53 - __main__ - main - INFO - Using Device: cuda:0\n",
      "2024-12-03 17:36:53 - __main__ - main - INFO - Loading preprocessed tensors from disk.\n",
      "2024-12-03 17:36:53 - __main__ - load_tensor - INFO - Loaded tensor from preprocessed_tensors/train_route_images_tensor.pt with shape torch.Size([3933, 64, 64])\n",
      "2024-12-03 17:36:53 - __main__ - load_tensor - INFO - Loaded tensor from preprocessed_tensors/train_additional_features_tensor.pt with shape torch.Size([3933, 28])\n",
      "2024-12-03 17:36:53 - __main__ - load_tensor - INFO - Loaded tensor from preprocessed_tensors/train_labels_tensor.pt with shape torch.Size([3933])\n",
      "2024-12-03 17:36:53 - __main__ - load_tensor - INFO - Loaded tensor from preprocessed_tensors/val_route_images_tensor.pt with shape torch.Size([492, 64, 64])\n",
      "2024-12-03 17:36:53 - __main__ - load_tensor - INFO - Loaded tensor from preprocessed_tensors/val_additional_features_tensor.pt with shape torch.Size([492, 28])\n",
      "2024-12-03 17:36:53 - __main__ - load_tensor - INFO - Loaded tensor from preprocessed_tensors/val_labels_tensor.pt with shape torch.Size([492])\n",
      "2024-12-03 17:36:53 - __main__ - load_tensor - INFO - Loaded tensor from preprocessed_tensors/test_route_images_tensor.pt with shape torch.Size([492, 64, 64])\n",
      "2024-12-03 17:36:53 - __main__ - load_tensor - INFO - Loaded tensor from preprocessed_tensors/test_additional_features_tensor.pt with shape torch.Size([492, 28])\n",
      "2024-12-03 17:36:53 - __main__ - load_tensor - INFO - Loaded tensor from preprocessed_tensors/test_labels_tensor.pt with shape torch.Size([492])\n",
      "2024-12-03 17:36:53 - TrafficDataset - __init__ - INFO - Loading preprocessed tensors.\n",
      "2024-12-03 17:36:53 - TrafficDataset - __init__ - INFO - Loaded all tensors successfully.\n",
      "2024-12-03 17:36:53 - TrafficDataset - __init__ - INFO - Number of samples: 3933\n",
      "2024-12-03 17:36:53 - TrafficDataset - __init__ - INFO - Loading preprocessed tensors.\n",
      "2024-12-03 17:36:53 - TrafficDataset - __init__ - INFO - Loaded all tensors successfully.\n",
      "2024-12-03 17:36:53 - TrafficDataset - __init__ - INFO - Number of samples: 492\n",
      "2024-12-03 17:36:53 - TrafficDataset - __init__ - INFO - Loading preprocessed tensors.\n",
      "2024-12-03 17:36:53 - TrafficDataset - __init__ - INFO - Loaded all tensors successfully.\n",
      "2024-12-03 17:36:53 - TrafficDataset - __init__ - INFO - Number of samples: 492\n",
      "2024-12-03 17:36:54 - __main__ - main - INFO - Initialized optimizer and scheduler.\n",
      "2024-12-03 17:36:54 - __main__ - main - INFO - Loaded saved model queue.\n",
      "2024-12-03 17:36:54 - __main__ - main - INFO - Loaded saved metrics queue.\n",
      "2024-12-03 17:37:05 - __main__ - main - INFO - Epoch [1/10], Loss: 1.1379, Accuracy: 35.93%\n",
      "2024-12-03 17:37:05 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:37:05 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:37:05 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:37:05 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.44      0.21      0.29       196\n",
      "       Light       0.44      0.22      0.30       241\n",
      "      Medium       0.12      0.58      0.20        55\n",
      "\n",
      "    accuracy                           0.26       492\n",
      "   macro avg       0.33      0.34      0.26       492\n",
      "weighted avg       0.40      0.26      0.28       492\n",
      "\n",
      "2024-12-03 17:37:05 - __main__ - main - INFO - Validation Loss: 1.0978, Validation Accuracy: 26.02%\n",
      "2024-12-03 17:37:05 - __main__ - main - INFO - Learning rate adjusted to: 0.000971\n",
      "2024-12-03 17:37:05 - __main__ - main - INFO - Best model saved with Validation Loss: 1.0978\n",
      "2024-12-03 17:37:05 - __main__ - main - INFO - Metrics incrementally saved at 'metrics/metrics_2024-12-03_1737.pkl'.\n",
      "2024-12-03 17:37:05 - __main__ - main - INFO - Model saved with timestamp: models/model_2024-12-03_1737.pth\n",
      "2024-12-03 17:37:16 - __main__ - main - INFO - Epoch [2/10], Loss: 1.1186, Accuracy: 36.87%\n",
      "2024-12-03 17:37:16 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:37:16 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:37:16 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:37:16 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.40      0.33      0.36       196\n",
      "       Light       0.50      0.40      0.44       241\n",
      "      Medium       0.13      0.33      0.18        55\n",
      "\n",
      "    accuracy                           0.36       492\n",
      "   macro avg       0.34      0.35      0.33       492\n",
      "weighted avg       0.42      0.36      0.38       492\n",
      "\n",
      "2024-12-03 17:37:16 - __main__ - main - INFO - Validation Loss: 1.0998, Validation Accuracy: 36.18%\n",
      "2024-12-03 17:37:16 - __main__ - main - INFO - Learning rate adjusted to: 0.000970\n",
      "2024-12-03 17:37:16 - __main__ - main - INFO - No improvement in Validation Loss. Patience: 1/5\n",
      "2024-12-03 17:37:16 - __main__ - main - INFO - Metrics incrementally saved at 'metrics/metrics_2024-12-03_1737.pkl'.\n",
      "2024-12-03 17:37:16 - __main__ - main - INFO - Model saved with timestamp: models/model_2024-12-03_1737.pth\n",
      "2024-12-03 17:37:27 - __main__ - main - INFO - Epoch [3/10], Loss: 1.1148, Accuracy: 37.05%\n",
      "2024-12-03 17:37:27 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:37:27 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:37:27 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:37:27 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.40      0.31      0.35       196\n",
      "       Light       0.50      0.44      0.47       241\n",
      "      Medium       0.12      0.29      0.17        55\n",
      "\n",
      "    accuracy                           0.37       492\n",
      "   macro avg       0.34      0.35      0.33       492\n",
      "weighted avg       0.42      0.37      0.39       492\n",
      "\n",
      "2024-12-03 17:37:27 - __main__ - main - INFO - Validation Loss: 1.1086, Validation Accuracy: 37.20%\n",
      "2024-12-03 17:37:27 - __main__ - main - INFO - Learning rate adjusted to: 0.000970\n",
      "2024-12-03 17:37:27 - __main__ - main - INFO - No improvement in Validation Loss. Patience: 2/5\n",
      "2024-12-03 17:37:27 - __main__ - main - INFO - Metrics incrementally saved at 'metrics/metrics_2024-12-03_1737.pkl'.\n",
      "2024-12-03 17:37:27 - __main__ - main - INFO - Model saved with timestamp: models/model_2024-12-03_1737.pth\n",
      "2024-12-03 17:37:38 - __main__ - main - INFO - Epoch [4/10], Loss: 1.1095, Accuracy: 35.83%\n",
      "2024-12-03 17:37:38 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:37:38 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:37:38 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:37:38 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.41      0.34      0.37       196\n",
      "       Light       0.52      0.36      0.42       241\n",
      "      Medium       0.13      0.40      0.20        55\n",
      "\n",
      "    accuracy                           0.35       492\n",
      "   macro avg       0.35      0.36      0.33       492\n",
      "weighted avg       0.43      0.35      0.38       492\n",
      "\n",
      "2024-12-03 17:37:38 - __main__ - main - INFO - Validation Loss: 1.0978, Validation Accuracy: 35.37%\n",
      "2024-12-03 17:37:38 - __main__ - main - INFO - Learning rate adjusted to: 0.000971\n",
      "2024-12-03 17:37:38 - __main__ - main - INFO - Best model saved with Validation Loss: 1.0978\n",
      "2024-12-03 17:37:38 - __main__ - main - INFO - Metrics incrementally saved at 'metrics/metrics_2024-12-03_1737.pkl'.\n",
      "2024-12-03 17:37:38 - __main__ - main - INFO - Model saved with timestamp: models/model_2024-12-03_1737.pth\n",
      "2024-12-03 17:37:49 - __main__ - main - INFO - Epoch [5/10], Loss: 1.1119, Accuracy: 35.27%\n",
      "2024-12-03 17:37:49 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:37:49 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:37:49 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:37:49 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.40      0.36      0.38       196\n",
      "       Light       0.51      0.28      0.36       241\n",
      "      Medium       0.12      0.40      0.19        55\n",
      "\n",
      "    accuracy                           0.33       492\n",
      "   macro avg       0.34      0.35      0.31       492\n",
      "weighted avg       0.42      0.33      0.35       492\n",
      "\n",
      "2024-12-03 17:37:49 - __main__ - main - INFO - Validation Loss: 1.0964, Validation Accuracy: 32.72%\n",
      "2024-12-03 17:37:49 - __main__ - main - INFO - Learning rate adjusted to: 0.000971\n",
      "2024-12-03 17:37:49 - __main__ - main - INFO - Best model saved with Validation Loss: 1.0964\n",
      "2024-12-03 17:37:49 - __main__ - main - INFO - Metrics incrementally saved at 'metrics/metrics_2024-12-03_1737.pkl'.\n",
      "2024-12-03 17:37:49 - __main__ - main - INFO - Model saved with timestamp: models/model_2024-12-03_1737.pth\n",
      "2024-12-03 17:38:00 - __main__ - main - INFO - Epoch [6/10], Loss: 1.1050, Accuracy: 36.51%\n",
      "2024-12-03 17:38:00 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:38:00 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:38:00 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:38:00 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.39      0.38      0.39       196\n",
      "       Light       0.49      0.29      0.37       241\n",
      "      Medium       0.12      0.36      0.18        55\n",
      "\n",
      "    accuracy                           0.33       492\n",
      "   macro avg       0.34      0.34      0.31       492\n",
      "weighted avg       0.41      0.33      0.35       492\n",
      "\n",
      "2024-12-03 17:38:00 - __main__ - main - INFO - Validation Loss: 1.1010, Validation Accuracy: 33.33%\n",
      "2024-12-03 17:38:00 - __main__ - main - INFO - Learning rate adjusted to: 0.000970\n",
      "2024-12-03 17:38:00 - __main__ - main - INFO - No improvement in Validation Loss. Patience: 1/5\n",
      "2024-12-03 17:38:00 - __main__ - main - INFO - Metrics incrementally saved at 'metrics/metrics_2024-12-03_1738.pkl'.\n",
      "2024-12-03 17:38:00 - __main__ - main - INFO - Oldest metrics removed from disk: metrics/metrics_2024-12-03_1737.pkl\n",
      "2024-12-03 17:38:00 - __main__ - main - INFO - Model saved with timestamp: models/model_2024-12-03_1738.pth\n",
      "2024-12-03 17:38:00 - __main__ - main - INFO - Oldest model removed from disk: models/model_2024-12-03_1737.pth\n",
      "2024-12-03 17:38:10 - __main__ - main - INFO - Epoch [7/10], Loss: 1.0997, Accuracy: 37.05%\n",
      "2024-12-03 17:38:11 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:38:11 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:38:11 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:38:11 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.42      0.39      0.40       196\n",
      "       Light       0.56      0.37      0.44       241\n",
      "      Medium       0.10      0.29      0.15        55\n",
      "\n",
      "    accuracy                           0.37       492\n",
      "   macro avg       0.36      0.35      0.33       492\n",
      "weighted avg       0.45      0.37      0.39       492\n",
      "\n",
      "2024-12-03 17:38:11 - __main__ - main - INFO - Validation Loss: 1.1001, Validation Accuracy: 36.59%\n",
      "2024-12-03 17:38:11 - __main__ - main - INFO - Learning rate adjusted to: 0.000970\n",
      "2024-12-03 17:38:11 - __main__ - main - INFO - No improvement in Validation Loss. Patience: 2/5\n",
      "2024-12-03 17:38:11 - __main__ - main - INFO - Metrics incrementally saved at 'metrics/metrics_2024-12-03_1738.pkl'.\n",
      "2024-12-03 17:38:11 - __main__ - main - INFO - Model saved with timestamp: models/model_2024-12-03_1738.pth\n",
      "2024-12-03 17:38:21 - __main__ - main - INFO - Epoch [8/10], Loss: 1.1017, Accuracy: 35.93%\n",
      "2024-12-03 17:38:21 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:38:21 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:38:21 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:38:21 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.43      0.32      0.36       196\n",
      "       Light       0.51      0.31      0.38       241\n",
      "      Medium       0.11      0.42      0.18        55\n",
      "\n",
      "    accuracy                           0.32       492\n",
      "   macro avg       0.35      0.35      0.31       492\n",
      "weighted avg       0.43      0.32      0.35       492\n",
      "\n",
      "2024-12-03 17:38:21 - __main__ - main - INFO - Validation Loss: 1.0917, Validation Accuracy: 32.32%\n",
      "2024-12-03 17:38:21 - __main__ - main - INFO - Learning rate adjusted to: 0.000971\n",
      "2024-12-03 17:38:21 - __main__ - main - INFO - Best model saved with Validation Loss: 1.0917\n",
      "2024-12-03 17:38:21 - __main__ - main - INFO - Metrics incrementally saved at 'metrics/metrics_2024-12-03_1738.pkl'.\n",
      "2024-12-03 17:38:21 - __main__ - main - INFO - Model saved with timestamp: models/model_2024-12-03_1738.pth\n",
      "2024-12-03 17:38:30 - __main__ - main - INFO - Epoch [9/10], Loss: 1.0990, Accuracy: 35.49%\n",
      "2024-12-03 17:38:31 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:38:31 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:38:31 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:38:31 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.42      0.27      0.33       196\n",
      "       Light       0.56      0.41      0.47       241\n",
      "      Medium       0.13      0.44      0.20        55\n",
      "\n",
      "    accuracy                           0.36       492\n",
      "   macro avg       0.37      0.37      0.33       492\n",
      "weighted avg       0.46      0.36      0.39       492\n",
      "\n",
      "2024-12-03 17:38:31 - __main__ - main - INFO - Validation Loss: 1.0926, Validation Accuracy: 35.77%\n",
      "2024-12-03 17:38:31 - __main__ - main - INFO - Learning rate adjusted to: 0.000971\n",
      "2024-12-03 17:38:31 - __main__ - main - INFO - No improvement in Validation Loss. Patience: 1/5\n",
      "2024-12-03 17:38:31 - __main__ - main - INFO - Metrics incrementally saved at 'metrics/metrics_2024-12-03_1738.pkl'.\n",
      "2024-12-03 17:38:31 - __main__ - main - INFO - Model saved with timestamp: models/model_2024-12-03_1738.pth\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Epoch [10/10], Loss: 1.0960, Accuracy: 37.05%\n",
      "2024-12-03 17:38:40 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:38:40 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:38:40 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:38:40 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.43      0.28      0.34       196\n",
      "       Light       0.55      0.39      0.45       241\n",
      "      Medium       0.16      0.55      0.24        55\n",
      "\n",
      "    accuracy                           0.36       492\n",
      "   macro avg       0.38      0.40      0.34       492\n",
      "weighted avg       0.46      0.36      0.38       492\n",
      "\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Validation Loss: 1.0896, Validation Accuracy: 36.18%\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Learning rate adjusted to: 0.000971\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Best model saved with Validation Loss: 1.0896\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Metrics incrementally saved at 'metrics/metrics_2024-12-03_1738.pkl'.\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Model saved with timestamp: models/model_2024-12-03_1738.pth\n",
      "/home/smebellis/ece5831_demo/src/train.py:617: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n",
      "2024-12-03 17:38:40 - __main__ - main - ERROR - Failed to load the best model from 'checkpoints/traffic_status_cnn_best.pth': [Errno 2] No such file or directory: 'checkpoints/traffic_status_cnn_best.pth'\n",
      "2024-12-03 17:38:40 - __main__ - evaluate - INFO - Unique labels in ground truth: [0 1 2]\n",
      "2024-12-03 17:38:40 - __main__ - evaluate - INFO - Unique labels in predictions: [0 1 2]\n",
      "2024-12-03 17:38:40 - __main__ - get_class_names - INFO - Class names retrieved from LabelEncoder: ['Heavy' 'Light' 'Medium']\n",
      "2024-12-03 17:38:40 - __main__ - evaluate - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Heavy       0.42      0.24      0.31       197\n",
      "       Light       0.45      0.36      0.40       241\n",
      "      Medium       0.08      0.30      0.13        54\n",
      "\n",
      "    accuracy                           0.30       492\n",
      "   macro avg       0.32      0.30      0.28       492\n",
      "weighted avg       0.40      0.30      0.33       492\n",
      "\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Test Loss: 1.1093, Test Accuracy: 30.28%\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - \n",
      "Test Performance Metrics:\n",
      "\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Accuracy: 30.284552845528456\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Precision: 0.39933917040793215\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Recall: 0.30284552845528456\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - F1 Score: 0.33162408036625013\n",
      "2024-12-03 17:38:40 - __main__ - main - INFO - Confusion Matrix:\n",
      "[[ 47  76  74]\n",
      " [ 55  86 100]\n",
      " [  9  29  16]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'src/train.py'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Training Script\n",
    "subprocess.run([\"python\", \"src/train.py\"], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
